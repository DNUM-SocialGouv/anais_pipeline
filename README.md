# Package Anais pipeline
Package Python de la pipeline d'exÃ©cution d'Anais

# Installation & Lancement du projet DBT

Cette section dÃ©crit les Ã©tapes nÃ©cessaires pour installer les dÃ©pendances et exÃ©cuter le projet.

---

## 1. Installation du package

Le projet utilise [UV] pour la gestion des dÃ©pendances Python.  
Voici les Ã©tapes Ã  suivre pour initialiser lâ€™environnement :

```bash

# 1. Se placer dans le dossier du projet
cd chemin/vers/le/projet

# 2. VÃ©rifier que uv est installÃ©
uv --version
pip install uv # Si pas installÃ©

# 3. Installer le package
uv uv pip install "git+https://github.com/ton-org/mon-package-pipeline.git@main"
```

---

## 2. Lancement de la pipeline :

L'ensemble de la Pipeline est exÃ©cutÃ© depuis le `main.py`.
La Pipeline est importÃ©e sous forme de package dans les diffÃ©rents projets.

### Pour l'exÃ©cution de la pipeline depuis un projet:
1. Placer vous dans le bon rÃ©pertoire `anais_<Nom_projet>`

```bash
# Placer vous dans anais_Nom_projet
cd anais_Nom_projet
```

2. Lancer le `main.py`
```bash
uv run -m pipeline.main --env "local" --profile "<Nom_projet>"
```
Avec `env = 'local'` ou `'anais'` selon votre environnement de travail
et `profile = '<Nom_projet>'`

## 3. Fonctionnement de la pipeline
### 3.1 Pour Staging
#### Pipeline Staging sur env 'local':
1. RÃ©cupÃ©ration des fichiers d'input. Ces fichiers doivent Ãªtre placÃ©s manuellement dans le dossier `input/` sous format **.csv** (les dÃ©limiteurs sont gÃ©rÃ©s automatiquement).
2. CrÃ©ation de la base DuckDB si inexistante.
3. Connexion Ã  la base DuckDB.
4. CrÃ©ation des tables, mÃªme si dÃ©jÃ  existantes. Les fichiers sql de crÃ©ation de table (CREATE TABLE) doivent Ãªtre placÃ©s dans le rÃ©pertoire indiquÃ© dans le `create_table_directory` de `metadata.yml`.
5. Lecture des csv avec standardisation des colonnes (ni caractÃ¨res spÃ©ciaux, ni majuscule) -> injection des donnÃ©es dans les tables.
6. Historisation des donnÃ©es pour chaque table vers les tables `z<nom_de_la_table` avec indication que la date d'injection dans la colonne `date_ingestion`.
7. VÃ©rification de la rÃ©ussite de l'injection.
8. Fermeture de la connexion Ã  la base DuckDB.
9. ExÃ©cution de la commande `run dbt` -> CrÃ©ation des vues relatives au projet.


#### Pipeline Staging sur env 'anais':
1. RÃ©cupÃ©ration des fichiers d'input depuis le SFTP. Ces fichiers sont placÃ©s dans le dossier `input/` sous format **.csv** (les dÃ©limiteurs sont gÃ©rÃ©s automatiquement).
2. Connexion Ã  la base Postgres.
3. CrÃ©ation des tables, mÃªme si dÃ©jÃ  existantes. Les fichiers sql de crÃ©ation de table (CREATE TABLE) doivent Ãªtre placÃ©s dans le rÃ©pertoire indiquÃ© dans le `create_table_directory` de `metadata.yml`.
4. Lecture des csv avec standardisation des colonnes (ni caractÃ¨res spÃ©ciaux, ni majuscule) -> injection des donnÃ©es dans les tables.
5. Historisation des donnÃ©es pour chaque table vers les tables `z<nom_de_la_table` avec indication que la date d'injection dans la colonne `date_ingestion`.
6. VÃ©rification de la rÃ©ussite de l'injection.
7. Fermeture de la connexion Ã  la base Postgres.
8. ExÃ©cution de la commande `run dbt` -> CrÃ©ation des vues relatives au projet.

### 3.2 Pour un autre projet
#### Pipeline <Nom_projet> sur env 'local':
1. CrÃ©ation de la base DuckDB si inexistante.
2. Connexion Ã  la base DuckDB.
3. MÃ©thode 1 : RÃ©cupÃ©ration des tables d'origine nÃ©cessaires Ã  <Nom_projet> Ã  partir de la base staging. 
3. MÃ©thode 2 : CrÃ©ation des tables d'origine nÃ©cessaires Ã  <Nom_projet> Ã  partir des fichiers CREATE TABLE .sql (`output_sql/<Nom_projet>/`) -> injection des donnÃ©es dans les tables Ã  partir des fichiers de donnÃ©es .csv (`input/<Nom_projet>/`).
4. Historisation des donnÃ©es pour chaque table vers les tables `z<nom_de_la_table` avec indication que la date d'injection dans la colonne `date_ingestion`.
5. VÃ©rification de la rÃ©ussite de l'injection.
7. Fermeture de la connexion Ã  la base DuckDB.
8. ExÃ©cution de la commande `run dbt` -> CrÃ©ation des vues relatives au projet.
9. Export des vues <Nom_projet> vers le rÃ©pertoire `output/<Nom_projet>/`.


#### Pipeline <Nom_projet> sur env 'anais':
1. Connexion Ã  la base Postgres.
2. RÃ©cupÃ©ration des tables d'origine nÃ©cessaires Ã  <Nom_projet> Ã  partir de la base staging.
3. Historisation des donnÃ©es pour chaque table vers les tables `z<nom_de_la_table` avec indication que la date d'injection dans la colonne `date_ingestion`.
4. ExÃ©cution de la commande `run dbt` -> CrÃ©ation des vues relatives au projet.
5. Export des vues <Nom_projet> vers le rÃ©pertoire `output/<Nom_projet>/`.
6. Export des vues <Nom_projet> vers le SFTP `/SCN_BDD/<Nom_projet>/output`.
7. Fermeture de la connexion Ã  la base Postgres.

## 4. Architecture du package

## ğŸ—ï¸ Architecture du projet

```plaintext
.
â”œâ”€â”€ pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ database_management
â”‚   â”‚   â”œâ”€â”€ database_pipeline.py
â”‚   â”‚   â”œâ”€â”€ duckdb_pipeline.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ postgres_loader.py
â”‚   â”œâ”€â”€ orchestration
â”‚   â”‚   â””â”€â”€ pipeline_orchestration.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ csv_management.py
â”‚       â”œâ”€â”€ dbt_tools.py
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ load_yml.py
â”‚       â”œâ”€â”€ logging_management.py
â”‚       â””â”€â”€ sftp_sync.py
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ profiles.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
```

## 5. UtilitÃ©s des fichiers
### 5.1 Fichiers Ã  la racine `./`
RÃ©pertoire d'orchestration de la pipeline Python.

- `pyproject.toml` : Fichier contenant les dÃ©pendances et packages nÃ©cessaires pour le lancement de la pipeline.
- `main.py` : Programme d'exÃ©cution de la pipeline.

### 5.2 Fichiers Ã  la racine `./pipeline/database_management/`
- `database_pipeline.py` : RÃ©alise les actions communes pour n'importe quelle database (lecture de fichier SQL, exÃ©cution du fichier sql, export de vues, run de la pipeline...). Fonctionne en complÃ©ment avec duckdb_pipeline.py et postgres_loader.py.
- `duckdb_pipeline.py` : RÃ©alise les actions spÃ©cifiques Ã  une base (connexion Ã  la base, crÃ©ation de table, chargement des donnÃ©es dans la BDD). Fonctionne en complÃ©ment avec database_pipeline.py.

- `postgres_loader.py` : RÃ©alise les actions spÃ©cifiques Ã  une base postgres (connexion Ã  la base, crÃ©ation de table, chargement des donnÃ©es dans la BDD). Fonctionne en complÃ©ment avec database_pipeline.py.

### 5.3 Fichiers Ã  la racine `./pipeline/orchestration/`
- `pipeline_orchestration.py` : Orchestre les diffÃ©rentes Ã©tapes de la pipeline selon l'environnement (local ou anais) et le projet (Staging ou autre).

### 5.4 Fichiers Ã  la racine `./pipeline/utils/`
- `config.py` : DÃ©finie les paramÃ¨tres de configuration liÃ©s aux logs, Ã  la database, Ã  l'heure d'exÃ©cution ...
- `csv_management.py` : RÃ©alise les actions relatives Ã  la manipulation de fichier csv (transformation d'un .xlsx en .csv, lecture du .csv avec dÃ©limiteur personnalisÃ©, standarisation des colonnes, conversion des types, export ...).
- `dbt_tools.py` : Permet de lancer des commandes dbt (dbt run, dbt test et dbt deps).
- `load_yml.py` : Permet la lecture d'un fichier .yaml
- `sftp_sync.py` : RÃ©alise les actions relatives Ã  une connexion SFTP (connexion, import, export...).
- `logging_management.py` : Initialise la log selon l'environnement.