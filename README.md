# Package Anais pipeline
Package Python de la pipeline d'ex√©cution d'Anais

# Installation & Lancement du projet DBT

Cette section d√©crit les √©tapes n√©cessaires pour installer les d√©pendances, configurer DBT, instancier la base de donn√©es si besoin, et ex√©cuter le projet.

---

## 1. Installation du package

Le projet utilise [UV] pour la gestion des d√©pendances Python.  
Voici les √©tapes √† suivre pour initialiser l‚Äôenvironnement :

```bash

# 1. Se placer dans le dossier du projet
cd chemin/vers/le/projet

# 2. V√©rifier que uv est install√©
uv --version
pip install uv # Si pas install√©

# 3. Installer le package
uv uv pip install "git+https://github.com/ton-org/mon-package-pipeline.git@main"
```

---

## 3. Lancement de la pipeline :

L'ensemble de la Pipeline est ex√©cut√© depuis le `main.py`.

### Pour l'ex√©cution de la pipeline:
1. Placer vous dans le bon r√©pertoire `anais_<Nom_projet>`

```bash
# Placer vous dans anais_Nom_projet
cd anais_Nom_projet
```

2. Lancer le `main.py`
```bash
uv run main.py --env "local" --profile "Nom_projet"
```
Avec env = 'local' ou 'anais' selon votre environnement de travail
et profile = 'Nom_projet'

## 4. Fonctionnement de la pipeline
### Pipeline sur env 'local':
1. R√©cup√©ration des fichiers d'input. !! Les fichiers doivent √™tre plac√©s manuellement dans le dossier `input/` sous format **.csv** !! (les d√©limiteurs sont g√©r√©s automatiquement)
2. Cr√©ation de la base DuckDB si inexistante.
3. Connexion √† la base DuckDB
4. Cr√©ation des tables si inexistantes
5. Lecture des csv avec standardisation des colonnes (sans caract√®res sp√©ciaux) -> injection des donn√©es dans les tables
6. V√©rification de l'injection
7. Ex√©cution de la commande `run dbt` -> Cr√©ation des vues relatives au projet
8. Export des vues dans le dossier `output/`
9. Fermeture de la connexion √† la base DuckDB

### Pipeline sur env 'anais':
1. R√©cup√©ration des fichiers d'input. Ces fichiers sont r√©cup√©r√©s automatiquement sur le SFTP et plac√©s dans le dossier `input/` sous format **.csv** (les d√©limiteurs sont g√©r√©s automatiquement)
2. Cr√©ation de la base Postgres si inexistante.
3. Connexion √† la base Postgres
4. Cr√©ation des tables si inexistantes
5. Lecture des csv avec standardisation des colonnes (sans caract√®res sp√©ciaux) -> injection des donn√©es dans les tables
6. V√©rification de l'injection
7. Ex√©cution de la commande `run dbt` -> Cr√©ation des vues relatives au projet
8. Export des vues dans le dossier `output/` au format **.csv**
9. Fermeture de la connexion √† la base Postgres
10. Export des **.csv** en output vers le SFTP


## 5. Architecture du package

## üèóÔ∏è Architecture du projet

```plaintext
.
‚îú‚îÄ‚îÄ data
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ duckdb_database.duckdb
‚îú‚îÄ‚îÄ input
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cert_dc_insern_2023_2024.csv
‚îÇ   ...
‚îÇ   ‚îî‚îÄ‚îÄ v_region.csv
‚îú‚îÄ‚îÄ logs
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sources.log
‚îú‚îÄ‚îÄ output
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ sa_ods_insee_2025_07_09.csv
‚îÇ   ...
‚îÇ   ‚îî‚îÄ‚îÄ sa_ods_pmsi_2025_07_09.csv
‚îú‚îÄ‚îÄ Staging
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dbtStaging
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ logs
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ macros
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ target
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tests
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ logs
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ output_sql
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cert_dc_insern_2023_2024.sql
‚îÇ¬†¬† ‚îÇ   ...
‚îÇ¬†¬† ‚îÇ   ‚îî‚îÄ‚îÄ v_region.sql
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pipeline
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ csv_management.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ database_pipeline.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ duckdb_pipeline.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ load_yml.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metadata.yml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ postgres_loader.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sftp_sync.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ staging_tables.txt
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ staging_views.txt
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ profiles.yml
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ uv.lock
```

## 6. Utilit√©s des fichiers
### ./dbtCertDC/
R√©pertoire de fonctionnement des mod√®les DBT -> cr√©ation de vue SQL.

dbt_project.yml : Fichier de configuration de DBT (obligatoire)

macros/ : R√©pertoire de stockage des macro jinja

models/ : R√©pertoire de stockage des mod√®les dbt

### ./pipeline/
R√©pertoire d'orchestration de la pipeline Python.

.env : Fichier secret contenant le param√©trage vers le SFTP et les mots de passes des bases de donn√©es postgres.

database_pipeline.py : R√©alise les actions communes pour n'importe quelle database (lecture de fichier SQL, ex√©cution du fichier sql, export de vues, run de la pipeline...). Fonctionne en compl√©ment avec duckdb_pipeline.py et postgres_loader.py.

duckdb_pipeline.py : R√©alise les actions sp√©cifiques √† une base (connexion √† la base, cr√©ation de table, chargement des donn√©es dans la BDD). Fonctionne en compl√©ment avec database_pipeline.py.

postgres_loader.py : R√©alise les actions sp√©cifiques √† une base postgres (connexion √† la base, cr√©ation de table, chargement des donn√©es dans la BDD). Fonctionne en compl√©ment avec database_pipeline.py.

sftp_sync.py : R√©alise les actions relatives √† une connexion SFTP (connexion, import, export...).

csv_management.py : R√©alise les actions relatives √† la manipulation de fichier csv (transformation d'un .xlsx en .csv, lecture du .csv avec d√©limiteur personnalis√©, standarisation des colonnes, conversion des types, export ...).

metadata.yml : Contient les informations relatives aux fichiers .csv provenant du SFTP.

load_yml.py : Lit un fichier .yml


./main.py : Programme d'ex√©cution de la pipeline


./output_sql/ : R√©pertoire qui contient les fichiers .sql de cr√©ation de table (CREATE TABLE)


./logs/ : R√©pertoire de la log


./data/duckdb_database.duckdb : Base duckDB


./input/ : R√©pertoire de stockage des fichiers .csv en entr√©e
./output/ : R√©pertoire de stockage des fichiers .csv en sortie


./profiles.yml : Contient les informations relatives aux bases des diff√©rents projets.


./poetry.lock
./uv.lock
./pyproject.toml